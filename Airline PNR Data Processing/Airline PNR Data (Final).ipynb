{"cells":[{"cell_type":"markdown","source":["### Bronze Layer For Airline PNR Data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"702a5c98-f482-436c-9f4b-b46f03ddb273"},{"cell_type":"code","source":["%pip install great_expectations"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:36:52.4117921Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"83543df2-ded1-4dee-9de1-3c5dd793bf3a"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting great_expectations\r\n  Downloading great_expectations-1.1.2-py3-none-any.whl (5.0 MB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]},{"output_type":"stream","name":"stdout","text":["\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m3.7/5.0 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hCollecting altair<5.0.0,>=4.2.1 (from great_expectations)\r\n  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/813.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: cryptography>=3.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (41.0.5)\r\nRequirement already satisfied: jinja2>=2.10 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (3.1.2)\r\nRequirement already satisfied: jsonschema>=2.5.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (4.19.1)\r\n"]},{"output_type":"stream","name":"stdout","text":["Collecting marshmallow<4.0.0,>=3.7.1 (from great_expectations)\r\n  Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\r\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25hRequirement already satisfied: mistune>=0.8.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (3.0.1)\r\nRequirement already satisfied: packaging in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (23.2)\r\nCollecting posthog<3,>=2.1.0 (from great_expectations)\r\n  Downloading posthog-2.5.0-py2.py3-none-any.whl (36 kB)\r\nRequirement already satisfied: pydantic>=1.10.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (1.10.9)\r\nRequirement already satisfied: pyparsing>=2.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (3.0.9)\r\nRequirement already satisfied: python-dateutil>=2.8.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (2.8.2)\r\nRequirement already satisfied: requests>=2.20 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (2.31.0)\r\nRequirement already satisfied: ruamel.yaml>=0.16 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (0.17.32)\r\nRequirement already satisfied: scipy>=1.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (1.10.1)\r\nRequirement already satisfied: tqdm>=4.59.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (4.66.1)\r\nRequirement already satisfied: typing-extensions>=4.1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (4.5.0)\r\n"]},{"output_type":"stream","name":"stdout","text":["Collecting tzlocal>=1.2 (from great_expectations)\r\n  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\r\nRequirement already satisfied: numpy>=1.22.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (1.24.3)\r\nRequirement already satisfied: pandas<2.2,>=1.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from great_expectations) (2.0.3)\r\nRequirement already satisfied: entrypoints in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\r\nRequirement already satisfied: toolz in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.12.0)\r\nRequirement already satisfied: cffi>=1.12 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from cryptography>=3.2->great_expectations) (1.16.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jinja2>=2.10->great_expectations) (2.1.3)\r\nRequirement already satisfied: attrs>=22.2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jsonschema>=2.5.1->great_expectations) (23.1.0)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jsonschema>=2.5.1->great_expectations) (2023.7.1)\r\nRequirement already satisfied: referencing>=0.28.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jsonschema>=2.5.1->great_expectations) (0.30.2)\r\nRequirement already satisfied: rpds-py>=0.7.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jsonschema>=2.5.1->great_expectations) (0.10.6)\r\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pandas<2.2,>=1.3.0->great_expectations) (2023.3.post1)\r\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pandas<2.2,>=1.3.0->great_expectations) (2023.3)\r\nRequirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from posthog<3,>=2.1.0->great_expectations) (1.16.0)\r\nRequirement already satisfied: monotonic>=1.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from posthog<3,>=2.1.0->great_expectations) (1.5)\r\nRequirement already satisfied: backoff>=1.10.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from posthog<3,>=2.1.0->great_expectations) (1.11.1)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (3.3.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (3.4)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (1.26.17)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\r\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.7)\r\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\r\n"]},{"output_type":"stream","name":"stdout","text":["Installing collected packages: tzlocal, marshmallow, posthog, altair, great_expectations\r\n  Attempting uninstall: posthog\r\n    Found existing installation: posthog 3.0.2\r\n    Not uninstalling posthog at /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages, outside environment /nfs4/pyenv-676514c5-7cd8-4c22-b542-f6b4826dbc20\r\n    Can't uninstall 'posthog'. No files were found to uninstall.\r\n"]},{"output_type":"stream","name":"stdout","text":["Successfully installed altair-4.2.2 great_expectations-1.1.2 marshmallow-3.22.0 posthog-2.5.0 tzlocal-5.2\r\n\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84b43ecd-c589-4aec-8d71-3bdd03085def"},{"cell_type":"markdown","source":["## Code For Just Streaming and Aggregation\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9f8f45b-72de-4bd4-b754-d1ed8d6da232"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, FloatType\n","from pyspark.sql.functions import col, from_json\n","\n","# Create Spark Session\n","spark = SparkSession.builder \\\n","    .appName(\"Kafka Customer and PNR Stream\") \\\n","    .getOrCreate()\n","\n","# Define schemas for customer and PNR data\n","customer_schema = StructType([\n","    StructField(\"customer_id\", StringType(), True),\n","    StructField(\"first_name\", StringType(), True),\n","    StructField(\"last_name\", StringType(), True),\n","    StructField(\"email\", StringType(), True),\n","    StructField(\"phone_number\", StringType(), True),\n","    StructField(\"passport_number\", StringType(), True),\n","    StructField(\"nationality\", StringType(), True)\n","])\n","\n","pnr_schema = StructType([\n","    StructField(\"pnr_id\", StringType(), True),\n","    StructField(\"customer_id\", StringType(), True),\n","    StructField(\"flight_number\", StringType(), True),\n","    StructField(\"departure_airport\", StringType(), True),\n","    StructField(\"arrival_airport\", StringType(), True),\n","    StructField(\"departure_date\", TimestampType(), True),\n","    StructField(\"ticket_price\", FloatType(), True),\n","    StructField(\"seat_number\", StringType(), True)\n","])\n","\n","custable = \"customer_table\"\n","deltaTablePath1 = \"Tables/\" + custable\n","\n","pnrtable = \"pnr_table\"\n","deltaTablePath2 = \"Tables/\" + pnrtable\n","\n","# Read customer data from Kafka topic\n","customers_df = spark.readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"158.220.124.0:9094\") \\\n","    .option(\"subscribe\", \"customers_data\") \\\n","    .option(\"startingOffsets\", \"latest\") \\\n","    .option(\"failOnDataLoss\", \"false\") \\\n","    .load() \\\n","    .selectExpr(\"CAST(value AS STRING) as json\") \\\n","    .select(from_json(col(\"json\"), customer_schema).alias(\"data\")) \\\n","    .select(\"data.*\")\n","\n","pnr_df = spark.readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"158.220.124.0:9094\") \\\n","    .option(\"subscribe\", \"pnr_data\") \\\n","    .option(\"startingOffsets\", \"latest\") \\\n","    .option(\"failOnDataLoss\", \"false\") \\\n","    .load() \\\n","    .selectExpr(\"CAST(value AS STRING) as json\") \\\n","    .select(from_json(col(\"json\"), pnr_schema).alias(\"data\")) \\\n","    .select(\"data.*\")\n","\n","ge_df = SparkDFDataset(customers_df)\n","# Example 1: Check that 'customer_id' column is unique\n","ge_df.expect_column_values_to_be_unique(\"customer_id\")\n","\n","# Write the customer DataFrame to a Delta table\n","query1 = customers_df.writeStream.outputMode(\"append\").format(\"delta\").option(\"path\", deltaTablePath1).option(\"mergeSchema\", \"true\").option(\"checkpointLocation\", deltaTablePath1 + \"/checkpoint\").start()\n","query2 = pnr_df.writeStream.outputMode(\"append\").format(\"delta\").option(\"path\", deltaTablePath2).option(\"mergeSchema\", \"true\").option(\"checkpointLocation\", deltaTablePath2 + \"/checkpoint\").start()\n","\n","# Create the aggregated table if it doesn't exist\n","spark.sql(\"\"\"\n","CREATE TABLE IF NOT EXISTS aggregated_table (\n","    unique_id STRING,\n","    customer_id_c STRING,\n","    customer_id_p STRING,\n","    first_name STRING,\n","    last_name STRING,\n","    pnr_id STRING,\n","    flight_number STRING,\n","    ticket_price FLOAT\n",") USING DELTA\n","\"\"\")\n","\n","while True:\n","    # Query your data\n","    df = spark.sql(\"\"\"\n","        SELECT c.customer_id AS customer_id_c, \n","               p.customer_id AS customer_id_p, \n","               c.first_name, \n","               c.last_name, \n","               p.pnr_id,\n","               p.flight_number,\n","               p.ticket_price\n","        FROM customer_table c \n","        INNER JOIN pnr_table p ON c.customer_id = p.customer_id\n","    \"\"\")\n","\n","    # Create a unique identifier for each record\n","    df_with_unique_id = df.withColumn(\"unique_id\", F.expr(\"concat(customer_id_c, pnr_id)\"))\n","\n","    # Deduplicate the source data to ensure only one row per unique_id\n","    df_with_unique_id_deduped = df_with_unique_id.dropDuplicates([\"unique_id\"])\n","\n","    # Perform the merge operation\n","    df_with_unique_id_deduped.createOrReplaceTempView(\"new_data\")\n","\n","    merge_query = \"\"\"\n","    MERGE INTO aggregated_table AS target\n","    USING new_data AS source\n","    ON target.unique_id = source.unique_id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.first_name = source.first_name,\n","            target.last_name = source.last_name,\n","            target.pnr_id = source.pnr_id,\n","            target.flight_number = source.flight_number,\n","            target.ticket_price = source.ticket_price\n","    WHEN NOT MATCHED THEN\n","        INSERT (unique_id, customer_id_c, customer_id_p, first_name, last_name, pnr_id, flight_number, ticket_price)\n","        VALUES (source.unique_id, source.customer_id_c, source.customer_id_p, source.first_name, source.last_name, source.pnr_id, source.flight_number, source.ticket_price)\n","    \"\"\"\n","\n","    # Execute the merge query\n","    spark.sql(merge_query)\n","\n","    time.sleep(30)  # Wait for 30 seconds before running the query again\n","\n","query1.awaitTermination()\n","query2.awaitTermination()\n","\n","#query1.awaitTermination()\n","#query2.awaitTermination()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:35:41.6638107Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"064fc23b-1fb2-47df-a735-e0e716e8fa36"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":28,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15454411-a8ab-4121-8d74-0ca90e57084c"},{"cell_type":"markdown","source":["## Code For Printing Kafka Data On Console"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6017faf1-b14b-429f-b0f2-78a202bc665d"},{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"Kafka Integration\") \\\n","    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n","    .getOrCreate()\n","\n","\n","# Kafka configuration\n","kafka_bootstrap_servers = \"158.220.124.0:9094\"  # Replace with your Kafka broker address\n","kafka_topic = \"customers_data\"                     # Replace with your Kafka topic  test_topic\n","\n","# Read from Kafka\n","df = spark \\\n","  .readStream \\\n","  .format(\"kafka\") \\\n","  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n","  .option(\"subscribe\", kafka_topic) \\\n","  .option(\"startingOffsets\", \"latest\") \\\n","  .option(\"failOnDataLoss\", \"false\")\\\n","  .load()\n","\n","# Convert Kafka message key and value from binary to string\n","kafka_df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n","\n","# Define a function to print each consumed message\n","def process_batch(df, batch_id):\n","    # Print the batch id and each row\n","    print(f\"Processing batch {batch_id}:\")\n","    for row in df.collect():\n","        print(f\"Key: {row['key']}, Value: {row['value']}\")\n","\n","# Output each message and write to console\n","query = kafka_df.writeStream \\\n","    .outputMode(\"append\") \\\n","    .format(\"console\") \\\n","    .foreachBatch(process_batch) \\\n","    .option(\"checkpointLocation\", \"Files/RealTime/checkpoint\") \\\n","    .start()\n","\n","query.awaitTermination()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:35:41.6646739Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"b6568478-b2fb-446e-a7ba-ce0761bd0f9b"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0db38a9b-2c37-46a1-98a9-bbff3e6709af"},{"cell_type":"markdown","source":["## Complete Code For Our Pipeline (Run only this Cell for Demo)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08cb5e58-16f2-4435-9ce8-63e8a77981fc"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, TimestampType, FloatType\n","from pyspark.sql.functions import col, from_json, window, current_timestamp\n","from pyspark.sql.functions import upper, regexp_replace, when, length\n","import great_expectations as ge\n","import os\n","import json\n","import pyspark.sql.functions as F\n","\n","\n","# Create Spark Session\n","spark = SparkSession.builder \\\n","    .appName(\"Kafka Customer and PNR Stream\") \\\n","    .getOrCreate()\n","\n","# Define schemas for customer and PNR data\n","customer_schema = StructType([\n","    StructField(\"customer_id\", StringType(), True),\n","    StructField(\"first_name\", StringType(), True),\n","    StructField(\"last_name\", StringType(), True),\n","    StructField(\"email\", StringType(), True),\n","    StructField(\"phone_number\", StringType(), True),\n","    StructField(\"passport_number\", StringType(), True),\n","    StructField(\"nationality\", StringType(), True)\n","])\n","\n","pnr_schema = StructType([\n","    StructField(\"pnr_id\", StringType(), True),\n","    StructField(\"customer_id\", StringType(), True),\n","    StructField(\"flight_number\", StringType(), True),\n","    StructField(\"departure_airport\", StringType(), True),\n","    StructField(\"arrival_airport\", StringType(), True),\n","    StructField(\"departure_date\", TimestampType(), True),\n","    StructField(\"ticket_price\", FloatType(), True),\n","    StructField(\"seat_number\", StringType(), True)\n","])\n","\n","custable = \"Uncleaned_Customer_Table\"\n","deltaTablePath1 = \"Tables/\" + custable\n","\n","cleancustable = \"Cleaned_Customer_Table\"\n","deltaTablePath2 = \"Tables/\" + cleancustable\n","\n","pnrtable = \"Uncleaned_Pnr_Table\"\n","deltaTablePath3 = \"Tables/\" + pnrtable\n","\n","cleanpnrtable = \"Cleaned_Pnr_Table\"\n","deltaTablePath4 = \"Tables/\" + cleanpnrtable\n","\n","# Read customer data from Kafka topic\n","customers_df = spark.readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"158.220.124.0:9094\") \\\n","    .option(\"subscribe\", \"customers_data\") \\\n","    .option(\"startingOffsets\", \"latest\") \\\n","    .option(\"failOnDataLoss\", \"false\") \\\n","    .load() \\\n","    .selectExpr(\"CAST(value AS STRING) as json\") \\\n","    .select(from_json(col(\"json\"), customer_schema).alias(\"data\")) \\\n","    .select(\"data.*\")\n","\n","pnr_df = spark.readStream \\\n","    .format(\"kafka\") \\\n","    .option(\"kafka.bootstrap.servers\", \"158.220.124.0:9094\") \\\n","    .option(\"subscribe\", \"pnr_data\") \\\n","    .option(\"startingOffsets\", \"latest\") \\\n","    .option(\"failOnDataLoss\", \"false\") \\\n","    .load() \\\n","    .selectExpr(\"CAST(value AS STRING) as json\") \\\n","    .select(from_json(col(\"json\"), pnr_schema).alias(\"data\")) \\\n","    .select(\"data.*\")\n","\n","cleaned_customer_df = customers_df \\\n","    .withColumn(\"customer_id\", regexp_replace(col(\"customer_id\"), \"[^A-Za-z0-9]\", \"\")) \\\n","    .withColumn(\"first_name\", regexp_replace(upper(col(\"first_name\")), \"[^A-Za-z]\", \"\")) \\\n","    .withColumn(\"last_name\", regexp_replace(upper(col(\"last_name\")), \"[^A-Za-z]\", \"\")) \\\n","    .withColumn(\"email\", when(col(\"email\").rlike(\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}$\"), col(\"email\")).otherwise(None)) \\\n","    .withColumn(\"phone_number\", regexp_replace(col(\"phone_number\"), \"[^0-9]\", \"\")) \\\n","    .withColumn(\"phone_number\", when(length(col(\"phone_number\")) >= 10, col(\"phone_number\")).otherwise(None)) \\\n","    .withColumn(\"passport_number\", regexp_replace(col(\"passport_number\"), \"[^A-Za-z0-9]\", \"\")) \\\n","    .withColumn(\"nationality\", regexp_replace(upper(col(\"nationality\")), \"[^A-Za-z]\", \"\")) \n","\n","cleaned_df = pnr_df \\\n","    .withColumn(\"pnr_id\", regexp_replace(col(\"pnr_id\"), \"[^A-Za-z0-9-]\", \"\")) \\\n","    .withColumn(\"customer_id\", regexp_replace(col(\"customer_id\"), \"[^A-Za-z0-9]\", \"\")) \\\n","    .withColumn(\"flight_number\", upper(col(\"flight_number\"))) \\\n","    .withColumn(\"flight_number\", regexp_replace(col(\"flight_number\"), \"[^A-Z0-9]\", \"\")) \\\n","    .withColumn(\"departure_airport\", upper(col(\"departure_airport\"))) \\\n","    .withColumn(\"arrival_airport\", upper(col(\"arrival_airport\"))) \\\n","    .withColumn(\"departure_airport\", when(col(\"departure_airport\").rlike(\"^[A-Z]{3}$\"), col(\"departure_airport\")).otherwise(None)) \\\n","    .withColumn(\"arrival_airport\", when(col(\"arrival_airport\").rlike(\"^[A-Z]{3}$\"), col(\"arrival_airport\")).otherwise(None)) \\\n","    .withColumn(\"departure_date\", when(col(\"departure_date\").isNotNull(), col(\"departure_date\")).otherwise(None)) \\\n","    .withColumn(\"ticket_price\", when(col(\"ticket_price\") >= 0, col(\"ticket_price\")).otherwise(None)) \\\n","    .withColumn(\"seat_number\", regexp_replace(col(\"seat_number\"), \"[^0-9A-Za-z]\", \"\")) \\\n","    .withColumn(\"seat_number\", when(col(\"seat_number\").rlike(\"^[0-9]+[A-Z]$\"), col(\"seat_number\")).otherwise(None))  # Example validation for seat number format\n","\n","# Adding window for micro batches\n","cleaned_customer_with_timestamp_df = cleaned_customer_df \\\n","    .withColumn(\"timestamp\", current_timestamp())\n","\n","windowed_customer_df = cleaned_customer_with_timestamp_df \\\n","    .withWatermark(\"timestamp\", \"10 minutes\") \\\n","    .groupBy(window(col(\"timestamp\"), \"1 minute\"), \"customer_id\") \\\n","    .count() \n","\n","cleaned_pnr_with_timestamp_df = cleaned_df \\\n","    .withColumn(\"timestamp\", current_timestamp())\n","\n","windowed_pnr_df = cleaned_pnr_with_timestamp_df \\\n","    .withWatermark(\"timestamp\", \"10 minutes\") \\\n","    .groupBy(window(col(\"timestamp\"), \"1 minute\"), \"pnr_id\") \\\n","    .count() \n","\n","\n","context = ge.get_context()\n","data_source = context.data_sources.add_spark(\"pandas\")\n","data_asset = data_source.add_dataframe_asset(name=\"pd dataframe asset\")\n","\n","batch_definition = data_asset.add_batch_definition_whole_dataframe(\"batch definition\")\n","batch = batch_definition.get_batch(batch_parameters={\"dataframe\": cleaned_customer_df})\n","# Convert Spark DataFrame to Pandas for validation with Great Expectations\n","\n","output_directory = \"abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/customer\"  # Change this to your lakehouse path\n","\n","# Ensure the output directory exists\n","os.makedirs(output_directory, exist_ok=True)\n","\n","output_directory2 = \"abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr\"  # Change this to your lakehouse path\n","\n","# Ensure the output directory exists\n","os.makedirs(output_directory2, exist_ok=True)\n","\n","# Validating the Cleaned Data using Great Expectations\n","#validate_id = ge.expectations.ExpectColumnDistinctValuesToBeInSet(\"customer_id\", cleaned_customer_df[\"customer_id\"].unique())\n","#validate_firstname = ge.expectations.ExpectColumnDistinctValuesToBeInSet(\"first_name\", cleaned_customer_df[\"first_name\"].unique())\n","#validate_lastname = ge.expectations.ExpectColumnDistinctValuesToBeInSet(\"last_name\", cleaned_customer_df[\"last_name\"].unique())\n","validate_email=ge.expectations.ExpectColumnValuesToMatchRegex(column =\"email\",regex= r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n","validate_number= ge.expectations.ExpectColumnValuesToMatchRegex(column =\"phone_number\",regex = r\"^\\+?[0-9]{10,15}$\")\n","validate_passport=ge.expectations.ExpectColumnValuesToMatchRegex(column =\"passport_number\", regex =r\"^[A-Z0-9]{5,9}$\")\n","#validate_nationality=ge.expectations.ExpectColumnDistinctValuesToBeInSet(\"nationality\", cleaned_customer_df[\"nationality\"].unique())\n","\n","# validations for PNR data\n","#pnridtoexist = ge.expectations.expect_column_to_exist(\"pnr_id\")\n","#cusidtoexist = ge.expectations.expect_column_to_exist(\"customer_id\")\n","#flightnumbertoexist = ge.expectations.expect_column_to_exist(\"flight_number\")\n","#departuretoexist = ge.expectations.expect_column_to_exist(\"departure_airport\")\n","#arrivaltoexist = ge.expectations.expect_column_to_exist(\"arrival_airport\")\n","#departuredatetoexist = ge.expectations.expect_column_to_exist(\"departure_date\")\n","#tickettoexist = ge.expectations.expect_column_to_exist(\"ticket_price\")\n","#seattoexist = ge.expectations.expect_column_to_exist(\"seat_number\")\n","\n","# Validations for pnr_id, customer_id, and flight_number (non-null)\n","pnridnotnull = ge.expectations.ExpectColumnValuesToNotBeNull(column=\"pnr_id\")\n","customeridnotnull = ge.expectations.ExpectColumnValuesToNotBeNull(column=\"customer_id\")\n","flightnumbernotnull = ge.expectations.ExpectColumnValuesToNotBeNull(column=\"flight_number\")\n","\n","# Validations for departure_airport and arrival_airport (non-null and within a specific list of valid airports)\n","#valid_airports = [\"JFK\", \"LAX\", \"ORD\", \"ATL\", \"DFW\"]  # Example airport codes\n","#ge_df.expect_column_values_to_not_be_null(\"departure_airport\")\n","#ge_df.expect_column_values_to_be_in_set(\"departure_airport\", valid_airports)\n","#ge_df.expect_column_values_to_not_be_null(\"arrival_airport\")\n","#ge_df.expect_column_values_to_be_in_set(\"arrival_airport\", valid_airports)\n","\n","def customer_validation_great_expectations(df, batch_id):\n","    validation_results = []\n","\n","    # Create a Great Expectations batch\n","    context = ge.get_context()\n","    data_source = context.data_sources.add_spark(\"pandas\")\n","    data_asset = data_source.add_dataframe_asset(name=\"pd dataframe asset\")\n","\n","    batch_definition = data_asset.add_batch_definition_whole_dataframe(\"batch definition\")\n","    batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df})\n","    # Validate each expectation\n","    validation_result_email = batch.validate(validate_email)\n","    validation_result_number = batch.validate(validate_number)\n","    validation_result_passport = batch.validate(validate_passport)\n","\n","    #print(validation_result_email)\n","    validation_results.append({\n","        'batch_id': batch_id,\n","        'email_validation': validation_result_email['success'],\n","        'phone_number_validation': validation_result_number['success'],\n","        'passport_validation': validation_result_passport['success']\n","    })\n","\n","    # Define the file name for the JSON file\n","    json_file_path = os.path.join(output_directory, f\"validation_results_{batch_id}.json\")\n","\n","    # Write the validation results to a JSON file\n","    with open(json_file_path, 'w') as json_file:\n","        json.dump(validation_results, json_file, indent=4)\n","\n","    # Print processing status\n","    print(f\"Processed batch {batch_id} and saved results to {json_file_path}.\")\n","\n","def pnr_validation_great_expectation(df, batch_id):\n","    validation_results = []\n","\n","    # Create a Great Expectations batch\n","    context = ge.get_context()\n","    data_source = context.data_sources.add_spark(\"pandas\")\n","    data_asset = data_source.add_dataframe_asset(name=\"pd dataframe asset\")\n","\n","    batch_definition = data_asset.add_batch_definition_whole_dataframe(\"batch definition\")\n","    batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df})\n","    # Validate each expectation\n","    validation_result_pnr = batch.validate(pnridnotnull)\n","    validation_result_cus = batch.validate(customeridnotnull)\n","    validation_result_flight = batch.validate(flightnumbernotnull)\n","   # print(validation_result_pnr)\n","\n","    validation_results.append({\n","        'batch_id': batch_id,\n","        'pnr_id_validation': validation_result_pnr['success'],\n","        'customer_id_validation': validation_result_cus['success'],\n","        'flight_number_validation': validation_result_flight['success']\n","    })\n","\n","    # Define the file name for the JSON file\n","    json_file_path = os.path.join(output_directory2, f\"validation_results_{batch_id}.json\")\n","\n","    # Write the validation results to a JSON file\n","    with open(json_file_path, 'w') as json_file:\n","        json.dump(validation_results, json_file, indent=4)\n","\n","    # Print processing status\n","    print(f\"Processed batch {batch_id} and saved results to {json_file_path}.\")\n","\n","# Apply the validation function to each batch of streaming data\n","# Validating Our Dataframes\n","query5 = cleaned_customer_df.writeStream \\\n","    .outputMode(\"append\") \\\n","    .format(\"console\") \\\n","    .trigger(processingTime=\"60 seconds\") \\\n","    .foreachBatch(customer_validation_great_expectations) \\\n","    .option(\"checkpointLocation\", \"Files/RealTime/checkpoint\") \\\n","    .start()\n","query6 = cleaned_df.writeStream \\\n","    .outputMode(\"append\") \\\n","    .format(\"console\") \\\n","    .trigger(processingTime=\"60 seconds\") \\\n","    .foreachBatch(pnr_validation_great_expectation) \\\n","    .option(\"checkpointLocation\", \"Files/RealTime/checkpoint\") \\\n","    .start()\n","\n","# Write the customer DataFrame to a Delta table \n","# Creating Bronze Layer For Customer and PNR\n","query1 = customers_df.writeStream.outputMode(\"append\").format(\"delta\").option(\"path\", deltaTablePath1).option(\"mergeSchema\", \"true\").option(\"checkpointLocation\", deltaTablePath1 + \"/checkpoint\").start()\n","query3 = pnr_df.writeStream.outputMode(\"append\").format(\"delta\").option(\"path\", deltaTablePath3).option(\"mergeSchema\", \"true\").option(\"checkpointLocation\", deltaTablePath3 + \"/checkpoint\").start()\n","\n","# Creating Silver Layer \n","query2 = cleaned_customer_df.writeStream.outputMode(\"append\").format(\"delta\").option(\"path\", deltaTablePath2).option(\"mergeSchema\", \"true\").option(\"checkpointLocation\", deltaTablePath2 + \"/checkpoint\").start()\n","query4 = cleaned_df.writeStream.outputMode(\"append\").format(\"delta\").option(\"path\", deltaTablePath4).option(\"mergeSchema\", \"true\").option(\"checkpointLocation\", deltaTablePath4 + \"/checkpoint\").start()\n","\n","\n","#Creating Gold Layer\n","# Create the aggregated table if it doesn't exist\n","spark.sql(\"\"\"\n","CREATE TABLE IF NOT EXISTS gold_aggregated_table (\n","    unique_id STRING,\n","    customer_id_c STRING,\n","    customer_id_p STRING,\n","    first_name STRING,\n","    last_name STRING, \n","    email STRING,\n","    phone_number STRING,\n","    passport_number STRING,\n","    nationality STRING,\n","    pnr_id STRING,\n","    flight_number STRING,\n","    ticket_price FLOAT,\n","    departure_airport STRING,\n","    arrival_airport STRING,\n","    departure_date TIMESTAMP,\n","    seat_number STRING\n","\n",") USING DELTA\n","\"\"\")\n","spark.sql(\"\"\"\n","CREATE TABLE IF NOT EXISTS gold_pnr_summary (\n","    customer_id_c STRING,\n","    passenger_name STRING,\n","    pnr_id STRING,\n","    flight_number STRING,\n","    ticket_price FLOAT,\n","    seat_number STRING,\n","    departure_date TIMESTAMP\n",") USING DELTA\n","\"\"\")\n","\n","\n","spark.sql(\"\"\"\n","CREATE TABLE IF NOT EXISTS gold_passenger_summary (\n","Passenger_Id STRING,\n","Passenger_name STRING,\n","Total_ticket_price STRING,\n","Average_Ticket_Price STRING,\n","Total_Flights STRING\n",") USING DELTA \"\"\")\n","\n","\n","while True:\n","    # Query your data\n","    df = spark.sql(\"\"\"\n","        SELECT c.customer_id AS customer_id_c, \n","               p.customer_id AS customer_id_p, \n","               c.first_name, \n","               c.last_name, \n","               c.email,\n","               c.phone_number,\n","               c.passport_number,\n","               c.nationality,\n","               p.pnr_id,\n","               p.flight_number,\n","               p.ticket_price,\n","               p.departure_airport,\n","               p.arrival_airport,\n","               p.departure_date,\n","               p.seat_number\n","        FROM Cleaned_Customer_Table c \n","        INNER JOIN Cleaned_Pnr_Table p ON c.customer_id = p.customer_id\n","    \"\"\")\n","\n","    # Create a unique identifier for each record\n","    df_with_unique_id = df.withColumn(\"unique_id\", F.expr(\"concat(customer_id_c, pnr_id)\"))\n","\n","    # Deduplicate the source data to ensure only one row per unique_id\n","    df_with_unique_id_deduped = df_with_unique_id.dropDuplicates([\"unique_id\"])\n","\n","    # Perform the merge operation\n","    df_with_unique_id_deduped.createOrReplaceTempView(\"new_data\")\n","\n","    merge_query = \"\"\"\n","    MERGE INTO gold_aggregated_table AS target\n","    USING new_data AS source\n","    ON target.unique_id = source.unique_id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.first_name = source.first_name,\n","            target.last_name = source.last_name,\n","            target.pnr_id = source.pnr_id,\n","            target.flight_number = source.flight_number,\n","            target.ticket_price = source.ticket_price,\n","            target.email = source.email,\n","            target.phone_number = source.phone_number,\n","            target.passport_number = source.passport_number,\n","            target.nationality = source.nationality,\n","            target.departure_airport = source.departure_airport,\n","            target.arrival_airport = source.arrival_airport,\n","            target.departure_date = source.departure_date,\n","            target.seat_number = source.seat_number\n","    WHEN NOT MATCHED THEN\n","        INSERT (unique_id, customer_id_c, customer_id_p, first_name, last_name, pnr_id, flight_number, ticket_price)\n","        VALUES (source.unique_id, source.customer_id_c, source.customer_id_p, source.first_name, source.last_name, source.pnr_id, source.flight_number, source.ticket_price)\n","    \"\"\"\n","\n","    \n","    df2 = spark.sql(\"\"\"\n","        select\n","        customer_id_c,\n","        (first_name ||' '||last_name) as passenger_name,\n","        pnr_id,\n","        flight_number,\n","        ticket_price, \n","        seat_number, \n","        departure_date \n","        from gold_aggregated_table;\n","    \"\"\")\n","\n","    df2.createOrReplaceTempView(\"new_pnr_view\")\n","    merge_query2 = \"\"\"\n","    MERGE INTO gold_pnr_summary AS target\n","    USING new_pnr_view AS source\n","    ON target.pnr_id = source.pnr_id\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.customer_id_c = source.customer_id_c,\n","            target.passenger_name = source.passenger_name,\n","            target.pnr_id = source.pnr_id,\n","            target.flight_number = source.flight_number,\n","            target.ticket_price = source.ticket_price,\n","            target.seat_number = source.seat_number,\n","            target.departure_date = source.departure_date\n","    WHEN NOT MATCHED THEN\n","        INSERT (customer_id_c, passenger_name, pnr_id, flight_number, ticket_price, seat_number, departure_date)\n","        VALUES (source.customer_id_c,\n","                source.passenger_name,\n","                source.pnr_id, \n","                source.flight_number, \n","                source.ticket_price, \n","                source.seat_number, \n","                source.departure_date)\n","    \"\"\"\n","\n","    df3 = spark.sql(\"\"\"SELECT \n","        customer_id_c ,\n","        (first_name ||' '||last_name) as passenger_name ,\n","        sum(ticket_price) as Total_ticket_price,\n","        avg(ticket_price) as Average_Ticket_Price,\n","        count(*) as Total_Flights\n","        FROM \n","            gold_aggregated_table \n","        group by customer_id_c,\n","        (first_name ||' '||last_name); \n","        \"\"\")\n","\n","    df3.createOrReplaceTempView(\"new_passenger\")\n","\n","    merge_summary_query = \"\"\"\n","    MERGE INTO gold_passenger_summary AS target\n","    USING new_passenger AS source\n","    ON target.Passenger_Id = source.customer_id_c\n","    WHEN MATCHED THEN\n","        UPDATE SET \n","            target.passenger_name = source.passenger_name,\n","            target.Total_ticket_price = source.Total_ticket_price,\n","            target.Average_Ticket_Price = source.Average_Ticket_Price,\n","            target.Total_Flights = source.Total_Flights\n","    WHEN NOT MATCHED THEN\n","        INSERT (Passenger_Id, passenger_name, Total_ticket_price, Average_Ticket_Price, Total_Flights)\n","        VALUES (source.customer_id_c,\n","                source.passenger_name,\n","                source.Total_ticket_price, \n","                source.Average_Ticket_Price, \n","                source.Total_Flights)\n","        \"\"\"\n","\n","    spark.sql(merge_query2)\n","    # Execute the merge query\n","    spark.sql(merge_query)\n","    spark.sql(merge_summary_query)\n","\n","query.awaitTermination()\n","query1.awaitTermination()\n","query2.awaitTermination()\n","query3.awaitTermination()\n","query4.awaitTermination()\n","query5.awaitTermination()\n","query6.awaitTermination()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:37:18.5222331Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"9546a161-e119-458c-aeba-240516b4a59c"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmpr_ihim2l' for ephemeral docs site\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmpbmxx8fyr' for ephemeral docs site\n"]},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3b9b580ae94866ba635097cb533aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15185cbd80b8488ab4be4422fb39d733"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c81bbad1cc7e4289996cf243c104bae7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processed batch 577 and saved results to abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr/validation_results_577.json.\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmpyhv85q_4' for ephemeral docs site\n"]},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1a380a9570423da7eff5050f866727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db77705ef77f456e81acd2bad932c7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23930951088a46548f3ceb9211c7057c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processed batch 578 and saved results to abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr/validation_results_578.json.\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmpo7ikh7z6' for ephemeral docs site\n"]},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1b50b422f8c40de871c0a6247ddb61c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9578a58f3ef24c5b9cd08d53c0bb2e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e45e0b97e446d499e82005dce3aea3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processed batch 579 and saved results to abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr/validation_results_579.json.\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmpkmyus92g' for ephemeral docs site\n"]},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8051c57d7d0444a68e0f2e4957fffa90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8f907ed45f4b35973a6d7c78e9d1b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e54ef54190c4c9b91fb4ac181d763e0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processed batch 580 and saved results to abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr/validation_results_580.json.\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmpv5_bfdil' for ephemeral docs site\n"]},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa9e97c6bb04e528dc164fb6777e0f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bbc7eda2ef043b6a7176bb84ea6758f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d0723536f74c8db26bbb34258e4878"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processed batch 581 and saved results to abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr/validation_results_581.json.\n"]},{"output_type":"stream","name":"stdout","text":["Created temporary directory '/tmp/tmplnbpu3xi' for ephemeral docs site\n"]},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307933bc54c841d0bc9851cf34fc3314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c24353287e14416c8b5c1f86576ea22f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating Metrics:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6ec1f2f2e349a88c57e17205056335"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processed batch 582 and saved results to abfss://ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db@onelake.dfs.fabric.microsoft.com/f2d127ee-ca13-4665-b186-31be52593744/Files/Validations_Checks/pnr/validation_results_582.json.\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o326.sql.\n: org.apache.spark.SparkException: Job 1595 cancelled part of cancelled job group 11\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2871)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2746)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1187)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3009)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2998)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:988)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2418)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2439)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2483)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1029)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:409)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1028)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:484)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4244)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3472)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4234)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:642)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3472)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.convertDataFrameToAddFiles(DataSkippingReader.scala:1091)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.convertDataFrameToAddFiles$(DataSkippingReader.scala:1090)\n\tat org.apache.spark.sql.delta.Snapshot.convertDataFrameToAddFiles(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$anonfun$getDataSkippedFiles$2(DataSkippingReader.scala:850)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$anonfun$getDataSkippedFiles$1(DataSkippingReader.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.getDataSkippedFiles(DataSkippingReader.scala:828)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.getDataSkippedFiles$(DataSkippingReader.scala:824)\n\tat org.apache.spark.sql.delta.Snapshot.getDataSkippedFiles(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$anonfun$filesForScan$5(DataSkippingReader.scala:946)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.Snapshot.recordOperation(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.Snapshot.recordDeltaOperation(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.filesForScan(DataSkippingReader.scala:922)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.filesForScan$(DataSkippingReader.scala:867)\n\tat org.apache.spark.sql.delta.Snapshot.org$apache$spark$sql$delta$stats$DataSkippingReaderWithDeltaScanCache$$super$filesForScan(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.getDeltaScan$1(DataSkippingReaderWithDeltaScanCache.scala:29)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.$anonfun$filesForScan$1(DataSkippingReaderWithDeltaScanCache.scala:30)\n\tat org.apache.spark.sql.delta.stats.SharedDeltaScanInMemoryCache$$anon$3.filesForScan(DeltaScanCache.scala:250)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.filesForScan(DataSkippingReaderWithDeltaScanCache.scala:30)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.filesForScan$(DataSkippingReaderWithDeltaScanCache.scala:28)\n\tat org.apache.spark.sql.delta.Snapshot.filesForScan(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.$anonfun$filesForScan$1(PrepareDeltaScan.scala:130)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.withStatusCode(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.filesForScan(PrepareDeltaScan.scala:115)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.filesForScan$(PrepareDeltaScan.scala:110)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.filesForScan(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase$$anonfun$prepareDeltaScan$1.$anonfun$applyOrElse$1(PrepareDeltaScan.scala:148)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase$$anonfun$prepareDeltaScan$1.applyOrElse(PrepareDeltaScan.scala:148)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase$$anonfun$prepareDeltaScan$1.applyOrElse(PrepareDeltaScan.scala:143)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1278)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1275)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:527)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1250)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1250)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.plans.logical.AggregateLike.mapChildren(basicLogicalOperators.scala:1111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)\n\tat org.apache.spark.sql.delta.SubqueryTransformerHelper.transformWithSubqueries(SubqueryTransformerHelper.scala:43)\n\tat org.apache.spark.sql.delta.SubqueryTransformerHelper.transformWithSubqueries$(SubqueryTransformerHelper.scala:40)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.transformWithSubqueries(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.prepareDeltaScan(PrepareDeltaScan.scala:143)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.prepareDeltaScan$(PrepareDeltaScan.scala:137)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.prepareDeltaScan(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.apply(PrepareDeltaScan.scala:203)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.apply$(PrepareDeltaScan.scala:184)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.apply(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.apply(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:93)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:288)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:642)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:288)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:287)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:210)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:206)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:225)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:230)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSparkPlanned(QueryExecution.scala:247)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:307)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:363)\n\tat org.apache.spark.sql.execution.QueryExecution.explainStringLocal(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:709)\n\tat org.apache.spark.sql.Dataset.localCheckpoint(Dataset.scala:696)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.prepareSourceDFAndReturnMaterializeReason(MergeIntoMaterializeSource.scala:265)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.prepareSourceDFAndReturnMaterializeReason$(MergeIntoMaterializeSource.scala:239)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.prepareSourceDFAndReturnMaterializeReason(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:293)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:247)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:237)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:247)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:245)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runOrig$1(MergeIntoCommand.scala:238)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:103)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:91)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runOrig(MergeIntoCommand.scala:238)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:201)\n\tat org.apache.spark.sql.delta.sources.SQLConfUtils$.withGlutenDisabled(SQLConfUtils.scala:39)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:201)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:145)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:123)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat jdk.internal.reflect.GeneratedMethodAccessor365.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 427\u001b[0m\n\u001b[1;32m    425\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(merge_query2)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# Execute the merge query\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(merge_summary_query)\n\u001b[1;32m    430\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination()\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o326.sql.\n: org.apache.spark.SparkException: Job 1595 cancelled part of cancelled job group 11\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2871)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2746)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1187)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3009)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2998)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:988)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2418)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2439)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2483)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1029)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:409)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1028)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:484)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4244)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3472)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4234)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:642)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3472)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.convertDataFrameToAddFiles(DataSkippingReader.scala:1091)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.convertDataFrameToAddFiles$(DataSkippingReader.scala:1090)\n\tat org.apache.spark.sql.delta.Snapshot.convertDataFrameToAddFiles(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$anonfun$getDataSkippedFiles$2(DataSkippingReader.scala:850)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$anonfun$getDataSkippedFiles$1(DataSkippingReader.scala:848)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.getDataSkippedFiles(DataSkippingReader.scala:828)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.getDataSkippedFiles$(DataSkippingReader.scala:824)\n\tat org.apache.spark.sql.delta.Snapshot.getDataSkippedFiles(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$anonfun$filesForScan$5(DataSkippingReader.scala:946)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.Snapshot.recordOperation(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.Snapshot.recordDeltaOperation(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.filesForScan(DataSkippingReader.scala:922)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderBase.filesForScan$(DataSkippingReader.scala:867)\n\tat org.apache.spark.sql.delta.Snapshot.org$apache$spark$sql$delta$stats$DataSkippingReaderWithDeltaScanCache$$super$filesForScan(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.getDeltaScan$1(DataSkippingReaderWithDeltaScanCache.scala:29)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.$anonfun$filesForScan$1(DataSkippingReaderWithDeltaScanCache.scala:30)\n\tat org.apache.spark.sql.delta.stats.SharedDeltaScanInMemoryCache$$anon$3.filesForScan(DeltaScanCache.scala:250)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.filesForScan(DataSkippingReaderWithDeltaScanCache.scala:30)\n\tat org.apache.spark.sql.delta.stats.DataSkippingReaderWithDeltaScanCache.filesForScan$(DataSkippingReaderWithDeltaScanCache.scala:28)\n\tat org.apache.spark.sql.delta.Snapshot.filesForScan(Snapshot.scala:67)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.$anonfun$filesForScan$1(PrepareDeltaScan.scala:130)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.withStatusCode(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.filesForScan(PrepareDeltaScan.scala:115)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.filesForScan$(PrepareDeltaScan.scala:110)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.filesForScan(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase$$anonfun$prepareDeltaScan$1.$anonfun$applyOrElse$1(PrepareDeltaScan.scala:148)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase$$anonfun$prepareDeltaScan$1.applyOrElse(PrepareDeltaScan.scala:148)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase$$anonfun$prepareDeltaScan$1.applyOrElse(PrepareDeltaScan.scala:143)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1278)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1275)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:527)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1250)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1250)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.plans.logical.AggregateLike.mapChildren(basicLogicalOperators.scala:1111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)\n\tat org.apache.spark.sql.delta.SubqueryTransformerHelper.transformWithSubqueries(SubqueryTransformerHelper.scala:43)\n\tat org.apache.spark.sql.delta.SubqueryTransformerHelper.transformWithSubqueries$(SubqueryTransformerHelper.scala:40)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.transformWithSubqueries(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.prepareDeltaScan(PrepareDeltaScan.scala:143)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.prepareDeltaScan$(PrepareDeltaScan.scala:137)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.prepareDeltaScan(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.apply(PrepareDeltaScan.scala:203)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScanBase.apply$(PrepareDeltaScan.scala:184)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.apply(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.delta.stats.PrepareDeltaScan.apply(PrepareDeltaScan.scala:282)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:93)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:214)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:288)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:642)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:288)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:287)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:210)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:206)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:225)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:230)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:227)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSparkPlanned(QueryExecution.scala:247)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:307)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:363)\n\tat org.apache.spark.sql.execution.QueryExecution.explainStringLocal(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:108)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:709)\n\tat org.apache.spark.sql.Dataset.localCheckpoint(Dataset.scala:696)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.prepareSourceDFAndReturnMaterializeReason(MergeIntoMaterializeSource.scala:265)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.prepareSourceDFAndReturnMaterializeReason$(MergeIntoMaterializeSource.scala:239)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.prepareSourceDFAndReturnMaterializeReason(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:293)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:247)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:237)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:247)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:245)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runOrig$1(MergeIntoCommand.scala:238)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:103)\n\tat org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:91)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:83)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runOrig(MergeIntoCommand.scala:238)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:201)\n\tat org.apache.spark.sql.delta.sources.SQLConfUtils$.withGlutenDisabled(SQLConfUtils.scala:39)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:201)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:152)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:145)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:145)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:123)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat jdk.internal.reflect.GeneratedMethodAccessor365.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]},{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:37:53.6035396Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"2ec5fd07-fd5b-4caf-be36-ab568346273b"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:39:06.6261387Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"5ab85e15-b5bf-4026-ba5b-6bde57c79a97"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:42:56.2429491Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"5cd1c31c-548d-47a2-852d-47f609fda107"},"text/plain":"StatementMeta(, , -1, Waiting, , Waiting)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0663eb47-4825-46c1-8a09-07f5779e1bf4"},{"cell_type":"markdown","source":["## SQL Queries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c07a9bb-25f1-48ac-9c1a-26beacfe12df"},{"cell_type":"code","source":["%%sql\n","--select count(*) from Cleaned_Customer_Table c  group by customer_id;\n","--select * from gold_aggregated_table ;\n","--select pnr_id, (first_name ||' '||last_name) as passenger_name,ticket_price, seat_number, departure_date from gold_aggregated_table;\n","-- select * from new_passenger_table where customer_id=10;\n","-- SELECT * from uncleaned_customer_table where customer_id=10;\n","-- SELECT * from cleaned_customer_table where customer_id=10;\n","SELECT * from uncleaned_pnr_table where customer_id=10;\n","SELECT * from cleaned_pnr_table where customer_id=10;\n","SELECT * from gold_passenger_summary where passenger_id=10;\n","\n","\n","\n","\n","--select * from gold_aggregated_table where customer_id_C =  '511';"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:49:28.0487638Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"37c01ae0-f81f-439a-a704-fb32bb231046"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"execute_result","execution_count":16,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"pnr_id","type":"string","nullable":true,"metadata":{}},{"name":"customer_id","type":"string","nullable":true,"metadata":{}},{"name":"flight_number","type":"string","nullable":true,"metadata":{}},{"name":"departure_airport","type":"string","nullable":true,"metadata":{}},{"name":"arrival_airport","type":"string","nullable":true,"metadata":{}},{"name":"departure_date","type":"timestamp","nullable":true,"metadata":{}},{"name":"ticket_price","type":"float","nullable":true,"metadata":{}},{"name":"seat_number","type":"string","nullable":true,"metadata":{}}]},"data":[["10-2","10","bW242","JFK","LHR","2024-09-01T15:17:25Z",321.79998779296875,"3F"],["10-2","10","bW242","JFK","LHR","2024-09-01T15:17:25Z",321.79998779296875,"3F"],["10-1","10","Ux380","DXB","LAX","2024-08-03T12:10:01Z",1284.9300537109375,"9C"]]},"text/plain":"<Spark SQL result set with 3 rows and 8 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":16,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"pnr_id","type":"string","nullable":true,"metadata":{}},{"name":"customer_id","type":"string","nullable":true,"metadata":{}},{"name":"flight_number","type":"string","nullable":true,"metadata":{}},{"name":"departure_airport","type":"string","nullable":true,"metadata":{}},{"name":"arrival_airport","type":"string","nullable":true,"metadata":{}},{"name":"departure_date","type":"timestamp","nullable":true,"metadata":{}},{"name":"ticket_price","type":"float","nullable":true,"metadata":{}},{"name":"seat_number","type":"string","nullable":true,"metadata":{}}]},"data":[["10-1","10","UX380","DXB","LAX","2024-08-03T12:10:01Z",1284.9300537109375,"9C"],["10-2","10","BW242","JFK","LHR","2024-09-01T15:17:25Z",321.79998779296875,"3F"],["10-2","10","BW242","JFK","LHR","2024-09-01T15:17:25Z",321.79998779296875,"3F"]]},"text/plain":"<Spark SQL result set with 3 rows and 8 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":16,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"Passenger_Id","type":"string","nullable":true,"metadata":{}},{"name":"Passenger_name","type":"string","nullable":true,"metadata":{}},{"name":"Total_ticket_price","type":"string","nullable":true,"metadata":{}},{"name":"Average_Ticket_Price","type":"string","nullable":true,"metadata":{}},{"name":"Total_Flights","type":"string","nullable":true,"metadata":{}}]},"data":[["10","HECTOR BATES","1606.7300415039062","803.3650207519531","2"]]},"text/plain":"<Spark SQL result set with 1 rows and 5 fields>"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"a020ca5e-b549-4666-9717-189f08b0acf1"},{"cell_type":"markdown","source":["## Example For Great Expectations Library"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d090d6e1-2971-41bf-8d33-e1b6f1b93727"},{"cell_type":"code","source":["import pandas as pd\n","import great_expectations as gx\n","from great_expectations.core.batch import Batch, BatchRequest\n","from great_expectations.validator.validator import Validator\n","\n","# Example DataFrame (Replace with your actual DataFrame)\n","data = {\n","    \"passenger_count\": [1, 3, 2, 6,  5],  # 10 is an outlier (greater than max 6)\n","    \"name\": [\"John\", \"Jane\", \"Doe\",  \"Bob\", \"Charlie\"]\n","}\n","\n","pandas_df = pd.DataFrame(data)\n","\n","# Step 1: Initialize Great Expectations context\n","context = gx.get_context()\n","data_source = context.data_sources.add_pandas(\"pandas\")\n","data_asset = data_source.add_dataframe_asset(name=\"pd dataframe asset\")\n","\n","batch_definition = data_asset.add_batch_definition_whole_dataframe(\"batch definition\")\n","batch = batch_definition.get_batch(batch_parameters={\"dataframe\": pandas_df})\n","\n","# Create Expectation.\n","expectation = gx.expectations.ExpectColumnValuesToBeBetween(\n","    column=\"passenger_count\", min_value=1, max_value=6\n",")\n","\n","# Validate Batch using Expectation.\n","validation_result = batch.validate(expectation)\n","\n","# Step 5: Validate the DataFrame with the expectation\n","\n","# Step 6: Print the validation results\n","print(validation_result)\n","\n","# Optional: Save the expectation suite or validation results\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-10-12T18:35:41.668466Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"bbef2bb9-d6ec-4060-bf60-8a7167d34abb"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26eb3ed5-0935-41e6-9aae-8b1108f3b068"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"version_major":2,"version_minor":0,"state":{"456eecc9b28d4327b51908899aedd995":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"9578a58f3ef24c5b9cd08d53c0bb2e18":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_475f9cdc530f4775b8487f1821702129","IPY_MODEL_86cdd945a60345c79d6819e4c5c37c9b","IPY_MODEL_6bfae40fa5834a1cafe1d6cecadf6ead"],"layout":"IPY_MODEL_1471883ae613460b863bd20083d2b6d6"}},"475f9cdc530f4775b8487f1821702129":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_c9df30124a494bfe897aba9b4c314814","style":"IPY_MODEL_c15486865e074de9a5fb9b254dc3d69d"}},"15185cbd80b8488ab4be4422fb39d733":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_f501fa5a94564b5ea151ed418ee0c6db","IPY_MODEL_cf6df51b76d3450886a94ae735fba4b6","IPY_MODEL_174ef2e2619f4c55940095fd5ef8b6b3"],"layout":"IPY_MODEL_9937d14194ec4f09977c210f589e4909"}},"a2002989bd3c42d280e93e374d926e4c":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"552eda7ed7764f0187afdf059ef1d2c1":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"44c4238e031644659b72bb6266d047f2":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"ce1f00804f624afd85a5ba847a99ba48":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"54e2edb2a275474a84a77f478a194c25":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_aeca2d424417426dabe9709a00d6e7f6","style":"IPY_MODEL_013c7cb3edf542558d7a9f164ad0f4b8"}},"61785aecfdd04e1da5f9483031927aae":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"9e54ef54190c4c9b91fb4ac181d763e0":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_625cbef9d49a469f8757ed67529bacfa","IPY_MODEL_b97fdd01debb4cb8a5de6a41254be24b","IPY_MODEL_3ff3af68b6b54be29c274196ab08fc38"],"layout":"IPY_MODEL_b29b1bb8ef544a8ba326771d29238171"}},"087c469ba5c34c3082b7ee6c9e660858":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"f156f2d90fbd40a286c01c42c73bdab7":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"822db31b085a42f78c10fdb805a67ac2":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"0b7dfc05d9794de19b425cb30ab6fd05":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"f501fa5a94564b5ea151ed418ee0c6db":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_61785aecfdd04e1da5f9483031927aae","style":"IPY_MODEL_0e266a7894d544cfabcabea29401d0b2"}},"7bb9da2083b94862b731b772e5c7e0ab":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"cf6df51b76d3450886a94ae735fba4b6":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_4dc8901a213a4266b8a1844eaeee704a","layout":"IPY_MODEL_4258039ae38448ba9a1bb4d1e9abc825"}},"78dec919b20643489ad16854444fdfee":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_2e1223c589e9479a86d2397186e3ec90","style":"IPY_MODEL_f23f032596064014ad8ab72a74436d7a"}},"b65a4385d10b4616a994ffa691411ca3":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"22aabe15e7054ed0923d579d57520a45":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_ff3680a97c8940b9b187ef5b95c5a684","layout":"IPY_MODEL_a09171dc4d34423aa79d8661fe9d5fee"}},"174ef2e2619f4c55940095fd5ef8b6b3":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 29.23it/s]","layout":"IPY_MODEL_30bdf0a540e24b39ae7d1dc67628aac2","style":"IPY_MODEL_b0e616953a4e4c4e8991b33b76d71f10"}},"5126ddf10a8247618acf0c208bd31291":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"b957e38dc058465692714695daec2e76":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"39ad74b1d9f84c8482c97f6f2fd90ea1":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"ecd6ef71af1e47bdbd53bff29c664cb4":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"c4133072145845d2b23d7fa9c99bb27d":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"fa86e6b96d014af0a4c7b1f48497b3b0":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_eca47e8e6d0d406b9ef3ff3f7bf394f8","layout":"IPY_MODEL_b074f4fe3e054c85b3953f2000a7468f"}},"1f9da2b5514f41fdb835474a7d66e7a1":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 70.56it/s]","layout":"IPY_MODEL_105e43bc2d0b42f1ad2a3748c0c5f89b","style":"IPY_MODEL_7d169d9a99944bd79bebc895e0326649"}},"55a92e6d5c384da3971c3920d8db1f73":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"30bdf0a540e24b39ae7d1dc67628aac2":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"44e26f957be14476b6653cc7f897564b":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"edaec27a1e8b4807b36a91936f9549de":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"a4b1f35e892243a78b3a31ac98f463e4":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:03&lt;00:00,  3.64it/s]","layout":"IPY_MODEL_5db317ac35f443b1a9a8e6ee39f8d746","style":"IPY_MODEL_b6df018e2b014067922418a6509608c7"}},"6b05f499051b4484b558eaad54c29d18":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"0ec933982b974ced80a8e9009c6f5789":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"16ed7b7a75934b93afc2af867800fa82":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_9f38f331dee4444585be3a625fb16810","style":"IPY_MODEL_3f95c64635fc4ff69dd3ac7680a5ca9c"}},"acea7739bc044c1295eaa5bd2037e21a":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"5bb54ad2f26d439fb2d14fc1889b6e5e":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_2032117e2ba348d19a4169a362bfbb84","style":"IPY_MODEL_df00f491b291432da99972aadc45e407"}},"db77705ef77f456e81acd2bad932c7cb":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_49bf8046836d4755b5a8cd50a6a003d1","IPY_MODEL_8a748403841c457dbd70709200007ff1","IPY_MODEL_fae4c0f4e7b741c9944b0390a92423e9"],"layout":"IPY_MODEL_d650e45fd6ea40ff9d6dec3bbbea31bc"}},"7d169d9a99944bd79bebc895e0326649":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"bf313914c1b1499e88ed51bc87f9c4b1":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"52e26bea095848268352ee790804c04b":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_b957e38dc058465692714695daec2e76","style":"IPY_MODEL_ff6724cb94594a31a185d16a37061a7f"}},"0559da1e36764c4f9a6a4938848062eb":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"8b33d97adcb34bd09ad094aab820edb4":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_260efc1f4f0748689966a76f00821a4c","layout":"IPY_MODEL_bc2d88822eb3418ea903fd879b642097"}},"d8f54327216342f4aac7c9d76c538ead":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"b3b3e4b32b0a4edaa1ca2b1d16e3f635":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"b97c2da7565e4a1ea3ad655a20eaf710":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 15.60it/s]","layout":"IPY_MODEL_7487a29626544e2a84f17716353052cc","style":"IPY_MODEL_b04c3ea7721049119d9e9ecbc5d00007"}},"16d908e7d8b24d1885c691c26d72b439":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:01&lt;00:00,  8.67it/s]","layout":"IPY_MODEL_822db31b085a42f78c10fdb805a67ac2","style":"IPY_MODEL_f26d5b46f8dd4a089fcd408db964b549"}},"b145fed0579a4f7183880b080d97953f":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"7bc005504aaa4aeaa4d2a9f4fce82ca2":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_0b7dfc05d9794de19b425cb30ab6fd05","style":"IPY_MODEL_f70952b9decd475ba1c9994b529e310c"}},"eca47e8e6d0d406b9ef3ff3f7bf394f8":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"8cb6632f050e4639be6cbabf55797656":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"8a748403841c457dbd70709200007ff1":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_3d774363445646d4b7ea1ae9a11cd804","layout":"IPY_MODEL_c3eb65ac32214838ac9fc8525525050b"}},"789248be379e48c4a7a7fd087ea5795a":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_d8f54327216342f4aac7c9d76c538ead","layout":"IPY_MODEL_62e0101dd1f14339bb4c514716e06eb6"}},"1fd8f09ac3104e71b3de6b6ffbd54282":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"26611535cd08470392e578389bdb8798":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"8051c57d7d0444a68e0f2e4957fffa90":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_d28b76faf3fc4eae8286f9f1888575c5","IPY_MODEL_789248be379e48c4a7a7fd087ea5795a","IPY_MODEL_84ca3913c42143eeadb758de6c9793ba"],"layout":"IPY_MODEL_45ae20d0cb1f40bc862383882909fe79"}},"c0544e04c8444bbf837d4903ce9ddd04":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_33c994e9a75f47de90091ca7389a85d5","layout":"IPY_MODEL_137efa150bb3498d86a1cf6d034db5a2"}},"260efc1f4f0748689966a76f00821a4c":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"04f02ed9c71a47e3a83e8c06f714d466":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"2cdd70e0ccd6458d8f99a128536875f9":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"49bf8046836d4755b5a8cd50a6a003d1":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_b3b3e4b32b0a4edaa1ca2b1d16e3f635","style":"IPY_MODEL_ecd6ef71af1e47bdbd53bff29c664cb4"}},"7e3d78992ac145a29d10d2a6eca41c8b":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"5a8f907ed45f4b35973a6d7c78e9d1b4":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_16ed7b7a75934b93afc2af867800fa82","IPY_MODEL_9a5db1cc76fa48b8809cbe220742d477","IPY_MODEL_94850375aabc4b2093be7d0add54f3a0"],"layout":"IPY_MODEL_e16282b03d28454995174a0b93b8ec3f"}},"df00f491b291432da99972aadc45e407":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"625cbef9d49a469f8757ed67529bacfa":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_0559da1e36764c4f9a6a4938848062eb","style":"IPY_MODEL_44e26f957be14476b6653cc7f897564b"}},"2ece15b59ed946238e7821be441debfa":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"23930951088a46548f3ceb9211c7057c":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_f9291871fbb54ab08866cb31e474d060","IPY_MODEL_201e366599084ee78401887f0fed5c56","IPY_MODEL_b97c2da7565e4a1ea3ad655a20eaf710"],"layout":"IPY_MODEL_43d28b17aed44877b57b224cc6c91736"}},"b04c3ea7721049119d9e9ecbc5d00007":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"c6cac9e44f41440e8788443d1157d5f4":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"45ae20d0cb1f40bc862383882909fe79":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"45550a9360574b8cb27fbe319565887f":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"f23f032596064014ad8ab72a74436d7a":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"0fb1b4d37abc42328b964b1d65147f49":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"95f0cb0145404f70b23a2d5ddc842cd7":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"dd3079ec090c478da22ce374b771c8a5":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"ad95a362b6fe415c8e963a935d479a0f":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"47d0723536f74c8db26bbb34258e4878":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_54e2edb2a275474a84a77f478a194c25","IPY_MODEL_6e3447c390be4f188c707b70e9d6b352","IPY_MODEL_23a6f334080943628cb2fc5f3d84b2aa"],"layout":"IPY_MODEL_180e4d9353b24986bf1494758f8552cc"}},"869643ab13e649b489d3d387ffcec5a7":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"ff3680a97c8940b9b187ef5b95c5a684":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"f70952b9decd475ba1c9994b529e310c":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"aeca2d424417426dabe9709a00d6e7f6":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"fe0cc2c33bd64728bcedbe48e431fb36":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"fcfc675f342d427e92d98f80f9314ce4":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_38207a8ed78542c5a2bf02c286467171","style":"IPY_MODEL_256f24bb5e3745ad874bd1f530cdcdeb"}},"c24353287e14416c8b5c1f86576ea22f":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_fcfc675f342d427e92d98f80f9314ce4","IPY_MODEL_616a9536bf7d474babccc5bfa8e798a0","IPY_MODEL_5db9899e046d434da81cbb1980b5d63a"],"layout":"IPY_MODEL_429f2e5c97064c7faf18807c31b6a97b"}},"2ba4bd261cc5454a8c31066890ae506c":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"ae353f98e0c94d25a9b2b942be5d2f00":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_5126ddf10a8247618acf0c208bd31291","layout":"IPY_MODEL_c4133072145845d2b23d7fa9c99bb27d"}},"201e366599084ee78401887f0fed5c56":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_95f0cb0145404f70b23a2d5ddc842cd7","layout":"IPY_MODEL_45550a9360574b8cb27fbe319565887f"}},"6bdc2e534b854b818ed4eab23718e585":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"e16282b03d28454995174a0b93b8ec3f":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"3d774363445646d4b7ea1ae9a11cd804":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"bec40ac8484d4a7cbf939a6a7b3a7687":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"d0ead563e21d4e0bbea4fdf250c320b0":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"e4acb98d4a39460e93fe4c4cfc85a2c3":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_2f0f0060b488401eb12a93c6be167aa7","layout":"IPY_MODEL_d6d07a286078452d83a3aafa6d2b1ceb"}},"bea2c48d23d04388b3beb42d538f972c":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"c3eb65ac32214838ac9fc8525525050b":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"5db9899e046d434da81cbb1980b5d63a":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 65.56it/s]","layout":"IPY_MODEL_d756207b0d7a43b18f2c864c1cd0adeb","style":"IPY_MODEL_0fb1b4d37abc42328b964b1d65147f49"}},"105e43bc2d0b42f1ad2a3748c0c5f89b":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"33274a84a3a04cc38f606864be438561":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_00192fd039f949e693ff811045f9d03b","style":"IPY_MODEL_4daa81a261c147ea980e1e956073e50a"}},"d6d07a286078452d83a3aafa6d2b1ceb":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"f1b50b422f8c40de871c0a6247ddb61c":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_3cd6f23d99974b46b118ad84d3673135","IPY_MODEL_22aabe15e7054ed0923d579d57520a45","IPY_MODEL_9da61e1e6fe2401b813cdad00ad1790d"],"layout":"IPY_MODEL_bf313914c1b1499e88ed51bc87f9c4b1"}},"fae4c0f4e7b741c9944b0390a92423e9":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 28.54it/s]","layout":"IPY_MODEL_f156f2d90fbd40a286c01c42c73bdab7","style":"IPY_MODEL_869643ab13e649b489d3d387ffcec5a7"}},"39b7eb7bb14f4059a52b4052ec3158c4":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 43.65it/s]","layout":"IPY_MODEL_552eda7ed7764f0187afdf059ef1d2c1","style":"IPY_MODEL_7e3d78992ac145a29d10d2a6eca41c8b"}},"6bfae40fa5834a1cafe1d6cecadf6ead":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 61.22it/s]","layout":"IPY_MODEL_fe0cc2c33bd64728bcedbe48e431fb36","style":"IPY_MODEL_2ece15b59ed946238e7821be441debfa"}},"e556bf02fd7c4eab891b76edca1e94ab":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"84ca3913c42143eeadb758de6c9793ba":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 14.40it/s]","layout":"IPY_MODEL_c7587b52463642fd816bf31e2fedc28e","style":"IPY_MODEL_bec40ac8484d4a7cbf939a6a7b3a7687"}},"a09171dc4d34423aa79d8661fe9d5fee":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"256f24bb5e3745ad874bd1f530cdcdeb":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"6e3447c390be4f188c707b70e9d6b352":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_31817bc704d44147899c32c50d4e442b","layout":"IPY_MODEL_a2002989bd3c42d280e93e374d926e4c"}},"a94f80b3201f49b3900c74476af1ce2f":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"3cd6f23d99974b46b118ad84d3673135":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_8cb6632f050e4639be6cbabf55797656","style":"IPY_MODEL_1fd8f09ac3104e71b3de6b6ffbd54282"}},"09e45e0b97e446d499e82005dce3aea3":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_d13dc7b43b644308a512918d4c43c8aa","IPY_MODEL_fa86e6b96d014af0a4c7b1f48497b3b0","IPY_MODEL_efb048554519414e913446bd0ac591e7"],"layout":"IPY_MODEL_ca896bde930d4314afa7887d38a87620"}},"f26d5b46f8dd4a089fcd408db964b549":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"429f2e5c97064c7faf18807c31b6a97b":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"33c994e9a75f47de90091ca7389a85d5":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"d756207b0d7a43b18f2c864c1cd0adeb":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"5db317ac35f443b1a9a8e6ee39f8d746":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"08a0fde6d4a54e3bb1656977dd6b42e4":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_7bb9da2083b94862b731b772e5c7e0ab","layout":"IPY_MODEL_39ad74b1d9f84c8482c97f6f2fd90ea1"}},"d28b76faf3fc4eae8286f9f1888575c5":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_087c469ba5c34c3082b7ee6c9e660858","style":"IPY_MODEL_c263958373c1482fa6c16c79e6bb57d8"}},"2032117e2ba348d19a4169a362bfbb84":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"2e1223c589e9479a86d2397186e3ec90":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"64694a865fc64a81bbcd60483d816e28":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"3f95c64635fc4ff69dd3ac7680a5ca9c":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"efb048554519414e913446bd0ac591e7":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 74.02it/s]","layout":"IPY_MODEL_d47563924bc84b1a92410cac7c17fa84","style":"IPY_MODEL_91ff611763db46949e9685e0c2e7187b"}},"f9291871fbb54ab08866cb31e474d060":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_ef661468c7ca41bcaa548194145b2687","style":"IPY_MODEL_6b05f499051b4484b558eaad54c29d18"}},"4dc8901a213a4266b8a1844eaeee704a":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"d13dc7b43b644308a512918d4c43c8aa":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_b6607bb4b8564d11a456dbe7e3393ddb","style":"IPY_MODEL_64a1a203d65c4e59ade5cb6daafbf0ac"}},"9a5db1cc76fa48b8809cbe220742d477":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_de037f54fa3648a4bf5afc6015b3f0e0","layout":"IPY_MODEL_15418401f44541f0a62b64e60ff9d61c"}},"38207a8ed78542c5a2bf02c286467171":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"43d28b17aed44877b57b224cc6c91736":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"0d6a873a64d946188703d0d5f925a669":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"85e1ffb340364f308ccf9921e256044b":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"15418401f44541f0a62b64e60ff9d61c":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"7487a29626544e2a84f17716353052cc":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"248a8eef1fd2432d8ef1a8490dc79bad":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"91ff611763db46949e9685e0c2e7187b":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"c7587b52463642fd816bf31e2fedc28e":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"ec3b9b580ae94866ba635097cb533aa4":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_88c87938cd514aacb8a9b91661c9fdd9","IPY_MODEL_43ea333d588941bb85c49f65caad3b16","IPY_MODEL_a4b1f35e892243a78b3a31ac98f463e4"],"layout":"IPY_MODEL_edaec27a1e8b4807b36a91936f9549de"}},"616a9536bf7d474babccc5bfa8e798a0":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_c6cac9e44f41440e8788443d1157d5f4","layout":"IPY_MODEL_a94f80b3201f49b3900c74476af1ce2f"}},"88c87938cd514aacb8a9b91661c9fdd9":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_50ade04baeb3465db96c530562f79c98","style":"IPY_MODEL_d0ead563e21d4e0bbea4fdf250c320b0"}},"b97fdd01debb4cb8a5de6a41254be24b":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_55a92e6d5c384da3971c3920d8db1f73","layout":"IPY_MODEL_7bcdbdcbf58a449b9d6d407c9145f956"}},"bc2d88822eb3418ea903fd879b642097":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"23a6f334080943628cb2fc5f3d84b2aa":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 57.99it/s]","layout":"IPY_MODEL_2ba4bd261cc5454a8c31066890ae506c","style":"IPY_MODEL_bea2c48d23d04388b3beb42d538f972c"}},"b7f64dd0839746458d3daa6dbd459b47":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"e2ca2f9fb27e4b56a54062d913451f16":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"62e0101dd1f14339bb4c514716e06eb6":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"61b47868fbf94cc891eaafe82b5a962a":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"fb6ec1f2f2e349a88c57e17205056335":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_78dec919b20643489ad16854444fdfee","IPY_MODEL_c0544e04c8444bbf837d4903ce9ddd04","IPY_MODEL_1f9da2b5514f41fdb835474a7d66e7a1"],"layout":"IPY_MODEL_64694a865fc64a81bbcd60483d816e28"}},"00192fd039f949e693ff811045f9d03b":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"d650e45fd6ea40ff9d6dec3bbbea31bc":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"9da61e1e6fe2401b813cdad00ad1790d":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:03&lt;00:00,  3.64it/s]","layout":"IPY_MODEL_dd3079ec090c478da22ce374b771c8a5","style":"IPY_MODEL_b7f64dd0839746458d3daa6dbd459b47"}},"de037f54fa3648a4bf5afc6015b3f0e0":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"bbb9e53fc0fc4dda8f9de6ace8ea8e76":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"b6607bb4b8564d11a456dbe7e3393ddb":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"aa466f24125b462c9cdd8d1ee77ec739":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 14.46it/s]","layout":"IPY_MODEL_248a8eef1fd2432d8ef1a8490dc79bad","style":"IPY_MODEL_b65a4385d10b4616a994ffa691411ca3"}},"9f38f331dee4444585be3a625fb16810":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"d47563924bc84b1a92410cac7c17fa84":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"137efa150bb3498d86a1cf6d034db5a2":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"2f0f0060b488401eb12a93c6be167aa7":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"50ade04baeb3465db96c530562f79c98":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"ff6724cb94594a31a185d16a37061a7f":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"c81bbad1cc7e4289996cf243c104bae7":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_5bb54ad2f26d439fb2d14fc1889b6e5e","IPY_MODEL_e4acb98d4a39460e93fe4c4cfc85a2c3","IPY_MODEL_39b7eb7bb14f4059a52b4052ec3158c4"],"layout":"IPY_MODEL_f9956bbd5cde442b8ae09edbd4498e00"}},"b29b1bb8ef544a8ba326771d29238171":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"31817bc704d44147899c32c50d4e442b":{"model_name":"ProgressStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":""}},"00fac84c4ab74648b71a20fc8b8275b5":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"1471883ae613460b863bd20083d2b6d6":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"f9956bbd5cde442b8ae09edbd4498e00":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"c9df30124a494bfe897aba9b4c314814":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"50f7651c8d134cea87a8741b082b4905":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":"Calculating Metrics: 100%","layout":"IPY_MODEL_44c4238e031644659b72bb6266d047f2","style":"IPY_MODEL_0ec933982b974ced80a8e9009c6f5789"}},"7bcdbdcbf58a449b9d6d407c9145f956":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"94850375aabc4b2093be7d0add54f3a0":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 80.38it/s]","layout":"IPY_MODEL_ad95a362b6fe415c8e963a935d479a0f","style":"IPY_MODEL_acea7739bc044c1295eaa5bd2037e21a"}},"43ea333d588941bb85c49f65caad3b16":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_e556bf02fd7c4eab891b76edca1e94ab","layout":"IPY_MODEL_0d6a873a64d946188703d0d5f925a669"}},"ef661468c7ca41bcaa548194145b2687":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"307933bc54c841d0bc9851cf34fc3314":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_33274a84a3a04cc38f606864be438561","IPY_MODEL_ae353f98e0c94d25a9b2b942be5d2f00","IPY_MODEL_bc6d4396ac0d41b4b27e12926e838cab"],"layout":"IPY_MODEL_26611535cd08470392e578389bdb8798"}},"3ff3af68b6b54be29c274196ab08fc38":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 39.89it/s]","layout":"IPY_MODEL_14a7c8cee60a4a18aa1ddb2cf4f51e27","style":"IPY_MODEL_e2ca2f9fb27e4b56a54062d913451f16"}},"ca896bde930d4314afa7887d38a87620":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"b6df018e2b014067922418a6509608c7":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"8f1a380a9570423da7eff5050f866727":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_7bc005504aaa4aeaa4d2a9f4fce82ca2","IPY_MODEL_8b33d97adcb34bd09ad094aab820edb4","IPY_MODEL_16d908e7d8b24d1885c691c26d72b439"],"layout":"IPY_MODEL_2cdd70e0ccd6458d8f99a128536875f9"}},"4daa81a261c147ea980e1e956073e50a":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"1bbc7eda2ef043b6a7176bb84ea6758f":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_50f7651c8d134cea87a8741b082b4905","IPY_MODEL_08a0fde6d4a54e3bb1656977dd6b42e4","IPY_MODEL_6375aad2d14147149d48ec265a6dc519"],"layout":"IPY_MODEL_04f02ed9c71a47e3a83e8c06f714d466"}},"86cdd945a60345c79d6819e4c5c37c9b":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_61b47868fbf94cc891eaafe82b5a962a","layout":"IPY_MODEL_40de92d8df3d4aa7b7379756b27ea9f6"}},"c15486865e074de9a5fb9b254dc3d69d":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"14a7c8cee60a4a18aa1ddb2cf4f51e27":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"4258039ae38448ba9a1bb4d1e9abc825":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"9937d14194ec4f09977c210f589e4909":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"6375aad2d14147149d48ec265a6dc519":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:00&lt;00:00, 75.84it/s]","layout":"IPY_MODEL_ce1f00804f624afd85a5ba847a99ba48","style":"IPY_MODEL_bbb9e53fc0fc4dda8f9de6ace8ea8e76"}},"c263958373c1482fa6c16c79e6bb57d8":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"40de92d8df3d4aa7b7379756b27ea9f6":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"bc6d4396ac0d41b4b27e12926e838cab":{"model_name":"HTMLModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":" 10/10 [00:01&lt;00:00, 10.02it/s]","layout":"IPY_MODEL_00fac84c4ab74648b71a20fc8b8275b5","style":"IPY_MODEL_b145fed0579a4f7183880b080d97953f"}},"b0e616953a4e4c4e8991b33b76d71f10":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"64a1a203d65c4e59ade5cb6daafbf0ac":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"013c7cb3edf542558d7a9f164ad0f4b8":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"180e4d9353b24986bf1494758f8552cc":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"0e266a7894d544cfabcabea29401d0b2":{"model_name":"HTMLStyleModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"description_width":"","font_size":null,"text_color":null}},"29289ce9abaf4397959b64ef17d195fd":{"model_name":"FloatProgressModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"value":10,"max":10,"bar_style":"success","style":"IPY_MODEL_85e1ffb340364f308ccf9921e256044b","layout":"IPY_MODEL_6bdc2e534b854b818ed4eab23718e585"}},"b074f4fe3e054c85b3953f2000a7468f":{"model_name":"LayoutModel","model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","state":{}},"1aa9e97c6bb04e528dc164fb6777e0f5":{"model_name":"HBoxModel","model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","state":{"children":["IPY_MODEL_52e26bea095848268352ee790804c04b","IPY_MODEL_29289ce9abaf4397959b64ef17d195fd","IPY_MODEL_aa466f24125b462c9cdd8d1ee77ec739"],"layout":"IPY_MODEL_456eecc9b28d4327b51908899aedd995"}}}}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f2d127ee-ca13-4665-b186-31be52593744","default_lakehouse_name":"AirlinePnr","default_lakehouse_workspace_id":"ed9bea8b-5f1a-4dbe-a49e-4cc0009f02db","known_lakehouses":[{"id":"f2d127ee-ca13-4665-b186-31be52593744"}]}}},"nbformat":4,"nbformat_minor":5}